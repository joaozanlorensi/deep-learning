{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zRQXVDpYnOyy"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\joaoz\\Desktop\\Repos\\deep-learning\\2 - Deep Feedforward Networks\\Assignment 2\\Assignment.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joaoz/Desktop/Repos/deep-learning/2%20-%20Deep%20Feedforward%20Networks/Assignment%202/Assignment.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#from sklearn.metrics import plot_confusion_matrix, confusion_matrix\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joaoz/Desktop/Repos/deep-learning/2%20-%20Deep%20Feedforward%20Networks/Assignment%202/Assignment.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joaoz/Desktop/Repos/deep-learning/2%20-%20Deep%20Feedforward%20Networks/Assignment%202/Assignment.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joaoz/Desktop/Repos/deep-learning/2%20-%20Deep%20Feedforward%20Networks/Assignment%202/Assignment.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoz/Desktop/Repos/deep-learning/2%20-%20Deep%20Feedforward%20Networks/Assignment%202/Assignment.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\distribute\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m sidecar_evaluator\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Python module for evaluation loop.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_logging \u001b[39mas\u001b[39;00m logging\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_column_lib \u001b[39mas\u001b[39;00m feature_column\n\u001b[0;32m     46\u001b[0m \u001b[39m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m module\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"FeatureColumns: tools for ingesting and representing features.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence_feature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m sparse_tensor_lib\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n\u001b[1;32m--> 143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m    144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m    145\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m check_ops\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy_tf_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m     18\u001b[0m InputSpec \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mInputSpec\n\u001b[0;32m     20\u001b[0m keras_style_scope \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mkeras_style_scope\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\models.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_module\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer_v1\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m functional\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Built-in activation functions.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m advanced_activations\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize_keras_object\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize_keras_object\n",
            "File \u001b[1;32mc:\\Users\\joaoz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py:22\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     19\u001b[0m \u001b[39m# Generic layers.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m InputLayer\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_spec\u001b[39;00m \u001b[39mimport\u001b[39;00m InputSpec\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pandas_profiling import ProfileReport\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 0: Read the train dataset and get a general idea on how it looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lZ-wmz-InOy7",
        "outputId": "5e3be5b3-8b88-4d8f-b7c6-d73a926d635b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Data/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPNjzZ-nOzA"
      },
      "source": [
        "# Step 1: Understand the general profile of this dataset and how consistently populated are its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGbO8ZyfnOzB"
      },
      "outputs": [],
      "source": [
        "#profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
        "#profile.to_file('RawDataProfile.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JTnT1dynOzC"
      },
      "outputs": [],
      "source": [
        "# There are many blanks in the 'Age' and in the 'Cabin' columns - Populating these features would be a bit troublesome - Discarding is an option for the Cabin column, since the amount of nulls is significantly higher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPg8HapRnOzC"
      },
      "source": [
        "# Step 2: Feature Engineering - Curating and adding new features from the raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn8c7jdLnOzD"
      },
      "outputs": [],
      "source": [
        "# Adding two new features: Family and Entitlement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWEfsBJynOzD"
      },
      "outputs": [],
      "source": [
        "df['Family'] = df['Name'].str.split(',').str.get(0)\n",
        "df['Entitlement'] = df['Name'].str.split(',').str.get(1).str.split(' ').str.get(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1HysZwXnOzE",
        "outputId": "25d0e024-13c7-4cfd-ec07-bc04966c8a63"
      },
      "outputs": [],
      "source": [
        "df['Entitlement'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_NJhUmAMnOzF",
        "outputId": "bb2f9a06-2028-4ba4-cfe7-b3ab1ea3abe0"
      },
      "outputs": [],
      "source": [
        "# Trying to get an idea of how many people there are in each family\n",
        "df[['Family', 'Name']].groupby('Family').size().reset_index(name='FamilySize').sort_values(by=['FamilySize'], ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "70bfaCQgnOzI",
        "outputId": "b1a58dca-0f75-492a-c3b2-feb6f74e306f"
      },
      "outputs": [],
      "source": [
        "# Trying to get an idea of how many people there are by entitlement\n",
        "df[['Entitlement', 'Name']].groupby('Entitlement').size().reset_index(name='qt').sort_values(by=['qt'], ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eD1LvAUonOzK",
        "outputId": "6e2f3dbd-186c-4ee2-f134-e343f22f9cd1"
      },
      "outputs": [],
      "source": [
        "# Trying to get an idea of how many people survived in each family\n",
        "df['Survived'] = df['Survived'].astype('int32')\n",
        "df[['Family', 'Survived']].groupby('Family').sum().reset_index().sort_values(by=['Survived'], ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LC2mL2gdnOzL",
        "outputId": "0540cd0d-5a68-4802-aa4f-9d40196a32e1"
      },
      "outputs": [],
      "source": [
        "# Trying to get an idea of how many people survived by each entitlement\n",
        "df[['Entitlement', 'Survived']].groupby('Entitlement').sum().reset_index().sort_values(by=['Survived'], ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ofl46GunOzP"
      },
      "outputs": [],
      "source": [
        "# Idea: add a column named 'FamilySize' to the original dataframe\n",
        "df_family = df[['Family', 'Name']].groupby('Family').size().reset_index(name='FamilySize')\n",
        "df = pd.merge(df, df_family, on='Family', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBiNXLs7nOzQ"
      },
      "outputs": [],
      "source": [
        "# Removing the 'Name' and the 'Family' columns now as they became unnecessary\n",
        "df.drop('Name', axis=1, inplace=True)\n",
        "df.drop('Family', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-8-EIBDBnOzR",
        "outputId": "97fffd75-8ca2-40f9-df26-8a73730d9630"
      },
      "outputs": [],
      "source": [
        "# Trying to understand the different types of families\n",
        "df[['SibSp', 'Parch', 'FamilySize']].drop_duplicates().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "SwsP0z7UnOzS",
        "outputId": "316fb9c6-fbee-40f9-a015-aae564d888ce"
      },
      "outputs": [],
      "source": [
        "df[['SibSp', 'Parch', 'FamilySize','PassengerId']].groupby(['SibSp', 'Parch', 'FamilySize']).count().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjOplxU8nOzT"
      },
      "outputs": [],
      "source": [
        "df['FamilyCategory'] = np.select(\n",
        "    [\n",
        "        (df['SibSp'] == 1) & (df['Parch'] == 0), \n",
        "        (df['SibSp'] == 0) & (df['Parch'] == 1),\n",
        "        (df['SibSp'] > 1 )& (df['Parch'] == 0),\n",
        "        (df['SibSp'] == 0) & (df['Parch'] > 1),\n",
        "        (df['SibSp'] == 0) & (df['Parch'] == 0) & (df['FamilySize'] != 1),\n",
        "        (df['SibSp'] == 0) & (df['Parch'] == 0) & (df['FamilySize'] == 1)\n",
        "    ], \n",
        "    [\n",
        "        'Couple', \n",
        "        'Couple',\n",
        "        'Couple and Children',\n",
        "        'Couple and Children',\n",
        "        'Relatives',\n",
        "        'Single person'\n",
        "    ], \n",
        "    default='Single person' # defaulting to 'Single Person' as most people were by themselves\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAnpoK_MnOzU"
      },
      "outputs": [],
      "source": [
        "# Filling in the null values for Age based in each family category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyIT4OXpnOzV",
        "outputId": "7b512993-923a-41af-cc50-7c509ba1546d"
      },
      "outputs": [],
      "source": [
        "df.loc[df['Age'].isna()].groupby('FamilyCategory').count()['PassengerId']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzTTmK6HnOzX",
        "outputId": "3e3cdd9c-1434-4c4c-b014-5990bcf3f669"
      },
      "outputs": [],
      "source": [
        "df.loc[~df['Age'].isna()].groupby('FamilyCategory').median()['Age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bxrWJIknOza"
      },
      "outputs": [],
      "source": [
        "df.loc[df.FamilyCategory.eq('Couple') & df.Age.isna()] = df.loc[df.FamilyCategory.eq('Couple') & df.Age.isna()].fillna(29)\n",
        "df.loc[df.FamilyCategory.eq('Couple and Children') & df.Age.isna()] = df.loc[df.FamilyCategory.eq('Couple and Children') & df.Age.isna()].fillna(26);\n",
        "df.loc[df.FamilyCategory.eq('Relatives') & df.Age.isna()] = df.loc[df.FamilyCategory.eq('Relatives') & df.Age.isna()].fillna(29);\n",
        "df.loc[df.FamilyCategory.eq('Single person') & df.Age.isna()] = df.loc[df.FamilyCategory.eq('Single person') & df.Age.isna()].fillna(28);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "l3CwFseanOzb",
        "outputId": "222ce8c2-d607-4876-a6f9-c0be833e828a"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fT07Id5qnOzc",
        "outputId": "2e571625-6526-4c15-8b5e-f6bd01159ca5"
      },
      "outputs": [],
      "source": [
        "df[['Fare', 'Cabin']].loc[~df['Cabin'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwAJYnbYnOzf"
      },
      "outputs": [],
      "source": [
        "# The 'Cabin' column is very badly populated. My decision will be to drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOFELIVOnOzg"
      },
      "outputs": [],
      "source": [
        "df.drop('Cabin', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "clQl4ulSnOzh",
        "outputId": "c4b28bd5-35c9-47d8-8cf3-62f58f5667a9"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHRxGgrMnOzh"
      },
      "outputs": [],
      "source": [
        "# The 'Ticket' column has a very high cardinality, making it difficult to use it for classification. My decision will also be to drop it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O69G0gh0nOzj",
        "outputId": "49f38292-cf36-4cf9-c8ac-f382a5ca63e9"
      },
      "outputs": [],
      "source": [
        "df.drop('Ticket', axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm-ZNdfAnOzl"
      },
      "outputs": [],
      "source": [
        "# Getting the data ready for training: Applying one-hot encoding to the categorical fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Ur1efUnOzm"
      },
      "outputs": [],
      "source": [
        "df = pd.get_dummies(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing an unnecessary entitlement\n",
        "df.drop('Entitlement_the', axis=1, inplace=True)\n",
        "# Adding a different entitlement which can be found on the test dataset\n",
        "df['Entitlement_Dona.'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRWIKwl5nOzk"
      },
      "outputs": [],
      "source": [
        "# Generating a new profiling report for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD3VZ_i4nOzk"
      },
      "outputs": [],
      "source": [
        "#profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
        "#profile.to_file('CuratedDataProfile.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCErfDWfnOzj"
      },
      "outputs": [],
      "source": [
        "# The data seems to be in a much better shape now for training a model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPqANL5knOzm"
      },
      "outputs": [],
      "source": [
        "X = df.drop('Survived', axis=1).copy()\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8VKXh2nOzn"
      },
      "outputs": [],
      "source": [
        "# Standardizing, casting and scaling the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXOPTX_GnOzn"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "input_dim = len(X_train.columns)\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train_arr = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test_arr = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4GXbbiCnOzy"
      },
      "source": [
        "# Step 3: Model Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3enq4gZnOz0"
      },
      "outputs": [],
      "source": [
        "# Functions to calculate accuracy metrics\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YnciTHonOz0"
      },
      "outputs": [],
      "source": [
        "# Trains a deep NN on the dataset\n",
        "def get_model(qt_relu_layers, optimizer, epochs):\n",
        "    model = Sequential()\n",
        "    # Rectified Linear Unit (ReLU) as the 1st Activation Function\n",
        "    # What it does is essentially outputting the input directly if it is positive, otherwise, it will output ero\n",
        "    model.add(Dense(qt_relu_layers, activation='relu', input_dim=input_dim))\n",
        "    # Intermediate layer\n",
        "    model.add(Dense(qt_relu_layers/2, activation='relu'))\n",
        "    # Softmax function as the last Activation Function \n",
        "    # What it does it essentially normalizing the output of a network to a probability distribution over the        redicted output classes\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy', f1_m])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T_sz_ZO5B6j"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzacXgYonOz1",
        "outputId": "e509cb24-5af1-49e9-bce0-66178207e9c6"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 20\n",
        "optimizer = 'RMSprop'\n",
        "model = get_model(batch_size, optimizer, epochs)\n",
        "history = model.fit(X_train_std, y_train_arr,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test_std, y_test_arr))\n",
        "score = model.evaluate(X_test_std, y_test_arr, verbose=0)\n",
        "print('-----------------------------------------------------')\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print('Test F1 score:', score[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OFkpAxAvnOz2",
        "outputId": "147ffee1-e87d-450f-bba2-0342b98b2bab"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy']) \n",
        "plt.title('Model Accuracy');\n",
        "plt.ylabel('Accuracy');\n",
        "plt.xlabel('Epoch'); \n",
        "plt.legend(['Train', 'Test'], loc='upper left');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANQ2D0eRnOz3"
      },
      "outputs": [],
      "source": [
        "# Using GridSearch to determine the best parameters and best train score (commented out to save computing resources when executing everything)\n",
        "'''\n",
        "param_grid = {'qt_relu_layers': [2, 4, 16, 32, 64, 128],\n",
        "            'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
        "            'epochs': [2, 5, 10, 20, 30, 40, 50, 100]\n",
        "}\n",
        "\n",
        "model = KerasClassifier(build_fn=get_model, verbose=False, batch_size=batch_size)\n",
        "\n",
        "use_all_processors = -1\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=use_all_processors)\n",
        "                                                  \n",
        "gs.fit(X_train, y_train_arr)\n",
        "\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV1_yVLk50XA"
      },
      "source": [
        "# Step 4: Training and evaluating the model that uses the best parameters found on the Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq5fzUNy4O4b",
        "outputId": "df9df857-e2ab-4877-f194-a046be95568b"
      },
      "outputs": [],
      "source": [
        "qt_relu_layers = 128\n",
        "epochs = 400\n",
        "optimizer = 'Adam'\n",
        "model = get_model(qt_relu_layers, optimizer, epochs)\n",
        "history = model.fit(X_train_std, y_train_arr,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_std, y_test_arr))\n",
        "score = model.evaluate(X_test_std, y_test_arr, verbose=0)\n",
        "print('-----------------------------------------------------')\n",
        "print('Test set loss:', score[0])\n",
        "print('Test set accuracy:', score[1])\n",
        "print('Test set F1 score:', score[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "jFflq9I1450F",
        "outputId": "c0150199-6321-4a05-9105-a92eda6be8b0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy']) \n",
        "plt.title('Model Accuracy');\n",
        "plt.ylabel('Accuracy');\n",
        "plt.xlabel('Epoch'); \n",
        "plt.legend(['Train', 'Test'], loc='upper left');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Confusion Matrix for the Test Data:\")\n",
        "y_test_arg = np.argmax(y_test_arr, axis=1)\n",
        "y_pred = np.argmax(model.predict(X_test_std), axis=1)\n",
        "print(confusion_matrix(y_test_arg, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Applying the model to the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val = pd.read_csv('Data/validation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val['Family'] = df_val['Name'].str.split(',').str.get(0)\n",
        "df_val['Entitlement'] = df_val['Name'].str.split(',').str.get(1).str.split(' ').str.get(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_family = df_val[['Family', 'Name']].groupby('Family').size().reset_index(name='FamilySize')\n",
        "df_val = pd.merge(df_val, df_family, on='Family', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing the 'Name' and the 'Family' columns now as they became unnecessary\n",
        "df_val.drop('Name', axis=1, inplace=True)\n",
        "df_val.drop('Family', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val[['SibSp', 'Parch', 'FamilySize','PassengerId']].groupby(['SibSp', 'Parch', 'FamilySize']).count().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val['FamilyCategory'] = np.select(\n",
        "    [\n",
        "        (df_val['SibSp'] == 1) & (df_val['Parch'] == 0), \n",
        "        (df_val['SibSp'] == 0) & (df_val['Parch'] == 1),\n",
        "        (df_val['SibSp'] > 1 )& (df_val['Parch'] == 0),\n",
        "        (df_val['SibSp'] == 0) & (df_val['Parch'] > 1),\n",
        "        (df_val['SibSp'] == 0) & (df_val['Parch'] == 0) & (df_val['FamilySize'] != 1),\n",
        "        (df_val['SibSp'] == 0) & (df_val['Parch'] == 0) & (df_val['FamilySize'] == 1)\n",
        "    ], \n",
        "    [\n",
        "        'Couple', \n",
        "        'Couple',\n",
        "        'Couple and Children',\n",
        "        'Couple and Children',\n",
        "        'Relatives',\n",
        "        'Single person'\n",
        "    ], \n",
        "    default='Single person' # defaulting to 'Single Person' as most people were by themselves\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzTTmK6HnOzX",
        "outputId": "3e3cdd9c-1434-4c4c-b014-5990bcf3f669"
      },
      "outputs": [],
      "source": [
        "df_val.loc[~df_val['Age'].isna()].groupby('FamilyCategory').median()['Age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bxrWJIknOza"
      },
      "outputs": [],
      "source": [
        "df_val.loc[df_val.FamilyCategory.eq('Couple') & df_val.Age.isna()] = df_val.loc[df_val.FamilyCategory.eq('Couple') & df_val.Age.isna()].fillna(30);\n",
        "df_val.loc[df_val.FamilyCategory.eq('Couple and Children') & df_val.Age.isna()] = df_val.loc[df_val.FamilyCategory.eq('Couple and Children') & df_val.Age.isna()].fillna(27);\n",
        "df_val.loc[df_val.FamilyCategory.eq('Relatives') & df_val.Age.isna()] = df_val.loc[df_val.FamilyCategory.eq('Relatives') & df_val.Age.isna()].fillna(24);\n",
        "df_val.loc[df_val.FamilyCategory.eq('Single person') & df_val.Age.isna()] = df_val.loc[df_val.FamilyCategory.eq('Single person') & df_val.Age.isna()].fillna(27);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOFELIVOnOzg"
      },
      "outputs": [],
      "source": [
        "df_val.drop('Cabin', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O69G0gh0nOzj",
        "outputId": "49f38292-cf36-4cf9-c8ac-f382a5ca63e9"
      },
      "outputs": [],
      "source": [
        "df_val.drop('Ticket', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val = pd.get_dummies(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val['Entitlement_Capt.'] = 0\n",
        "df_val['Entitlement_Don.'] = 0\n",
        "df_val['Entitlement_Mlle.'] = 0\n",
        "df_val['Entitlement_Mme.'] = 0\n",
        "df_val['Entitlement_Jonkheer.'] = 0\n",
        "df_val['Entitlement_Lady.'] = 0\n",
        "df_val['Entitlement_Major.'] = 0\n",
        "df_val['Entitlement_Sir.'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val = df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "sc.fit(X_val)\n",
        "X_val_std = sc.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc.fit(X)\n",
        "X_std = sc.transform(X)\n",
        "y_arr = keras.utils.np_utils.to_categorical(y, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using GridSearch to determine the best parameters and best train score (commented out to save computing resources when executing everything)\n",
        "'''\n",
        "param_grid = {\n",
        "  'qt_relu_layers': [2, 4, 16, 32, 64, 128],\n",
        "  'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
        "  'epochs': [2, 5, 10, 20, 30, 40, 50, 100, 200]\n",
        "}\n",
        "\n",
        "model = KerasClassifier(build_fn=get_model, verbose=False, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "use_all_processors = -1\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=use_all_processors)\n",
        "                                                  \n",
        "gs.fit(X_std, y_arr)\n",
        "\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training our model with the best NN\n",
        "qt_relu_layers = 128\n",
        "optimizer = 'RMSprop'\n",
        "epochs = 200\n",
        "model = get_model(qt_relu_layers, optimizer, epochs)\n",
        "model.fit(X_std, y_arr,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_val_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = pd.DataFrame(y_pred)\n",
        "y_pred.columns = ['Survived']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_results = pd.concat([X_val['PassengerId'], y_pred], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_results.to_csv('Results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Week4_Assignment.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
